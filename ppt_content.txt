MUDRA ANALYSIS - BHARATANATYAM ML MODEL
PPT PRESENTATION CONTENT

=================================================
SLIDE 1: INTRODUCTION
=================================================

PROJECT TITLE:
Mudra Analysis - Bharatanatyam Dance Performance Analysis using Machine Learning

OVERVIEW:
An intelligent system that automatically analyzes Bharatanatyam dance videos to detect hand gestures (mudras) and dance sequences, generating meaningful narratives about the performance.

KEY CAPABILITIES:
• Detects 50+ traditional hand gestures (mudras) from video frames
• Recognizes classical dance steps (Alarippu, Jathiswaram, Shabdam, Varnam, Padam, Tillana)
• Generates human-readable narratives explaining mudra meanings and cultural significance
• Provides real-time API for video analysis

TECHNOLOGY STACK:
• Deep Learning: TensorFlow, EfficientNetB0 CNN
• Computer Vision: MediaPipe, OpenCV
• API Framework: FastAPI
• Deployment: Render Cloud Platform

=================================================
SLIDE 2: LITERATURE SURVEY
=================================================

EXISTING RESEARCH:

1. Hand Gesture Recognition Systems
   - Traditional CV methods using HOG, SIFT features (Limited accuracy: 60-70%)
   - Deep learning approaches with CNNs (Improved accuracy: 85-90%)
   - Challenge: Real-world lighting and background variation

2. Dance Analysis Systems
   - Motion capture systems (Expensive, studio-limited)
   - Kinect-based pose estimation (Low resolution, depth sensor required)
   - RGB video analysis (Accessible but computationally intensive)

3. MediaPipe Framework
   - Google's real-time hand tracking solution
   - 21 3D landmarks per hand with 95%+ accuracy
   - Lightweight and suitable for mobile deployment

4. Transfer Learning with EfficientNets
   - State-of-the-art CNN architecture
   - Better accuracy with fewer parameters
   - Proven effective for fine-grained classification

RESEARCH GAP:
No existing system combines mudra detection with cultural context narration for Bharatanatyam performances.

=================================================
SLIDE 3: PROBLEM DEFINITION
=================================================

PROBLEM STATEMENT:
Traditional Bharatanatyam performances contain rich symbolic meanings through mudras, but understanding them requires expert knowledge. There is no automated system to:
1. Detect and classify mudras in dance videos
2. Explain their cultural and symbolic meanings
3. Provide temporal analysis of gesture sequences
4. Make dance analysis accessible to students and enthusiasts

CHALLENGES:

Technical Challenges:
• Hand gesture variability across performers
• Rapid transitions between mudras (< 1 second)
• Lighting conditions and camera angles
• Occlusion from body movements
• Similar-looking mudras (fine-grained classification)

Cultural Challenges:
• 50+ mudra classes with regional variations
• Context-dependent meanings
• Single-hand vs. double-hand mudra combinations

Performance Requirements:
• Real-time or near-real-time processing
• High accuracy (>90%) for reliable analysis
• Scalable API for multiple concurrent requests
• Low memory footprint for cloud deployment

=================================================
SLIDE 4: OBJECTIVES
=================================================

PRIMARY OBJECTIVES:

1. Mudra Detection System
   • Accurately classify 50+ Bharatanatyam mudras
   • Achieve >90% classification accuracy
   • Process video frames at 3-second intervals

2. Temporal Analysis
   • Track mudra sequences over time
   • Identify dance step patterns (Alarippu, Jathiswaram, etc.)
   • Generate frame-by-frame timeline

3. Narrative Generation
   • Map mudras to cultural meanings
   • Create human-readable performance descriptions
   • Provide statistical summary of mudra usage

4. Deployable API
   • RESTful API for video upload and analysis
   • Background task processing
   • JSON output with timestamps and confidence scores

SECONDARY OBJECTIVES:

• Dataset curation and augmentation
• Model optimization for cloud deployment
• Memory-efficient inference pipeline
• Support for multiple video formats (MP4, AVI, MOV, MKV, WEBM)

SUCCESS METRICS:

• Classification Accuracy: >90%
• Top-3 Accuracy: >95%
• Processing Speed: <1 minute per minute of video
• API Response Time: <3 seconds for status checks
• Memory Usage: <1GB per video analysis

=================================================
SLIDE 5: PROPOSED METHODOLOGY
=================================================

SYSTEM WORKFLOW:

Phase 1: Data Collection & Preprocessing
1. Dataset Acquisition
   - Kaggle 50 Mudras Dataset (primary source)
   - Asamyuktha 27 mudras (single-hand)
   - Custom Bharatanatyam video collection

2. Data Augmentation (3× multiplier)
   - Horizontal flipping (mirrored performances)
   - Rotation ±15° (wrist angle variations)
   - Brightness adjustment ±20% (lighting tolerance)

3. Feature Extraction
   - MediaPipe Hands: 21 landmarks × 2 hands × 3 coords = 126 features
   - MediaPipe Pose: 33 landmarks × 3 coords = 99 features
   - DeepFace Emotion: 1 categorical feature (7 emotions)

Phase 2: Model Development

1. Mudra Classification Model
   Architecture: EfficientNetB0 (Transfer Learning)
   - Base: Pre-trained ImageNet weights
   - Input: 224×224×3 RGB hand crops
   - Layers: GlobalAveragePooling2D → Dropout(0.2) → Dense(50, softmax)
   - Optimizer: Adam (lr=0.001)
   - Loss: Sparse Categorical Crossentropy

2. Training Strategy
   - Train/Val/Test Split: 70/15/15
   - Batch Size: 32
   - Epochs: 100 (with early stopping)
   - Callbacks: ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

3. Dance Step Prediction (Future)
   - LSTM/GRU for temporal sequence modeling
   - Sliding window: 30 frames (~1 second)
   - Combines hand + pose + emotion features

Phase 3: Inference Pipeline

1. Video Processing
   - Frame extraction every 3 seconds
   - Hand detection and cropping
   - Landmark extraction

2. Mudra Detection
   - Center-crop hand regions with 50% margin
   - Resize to 224×224
   - CNN inference → Top-3 predictions

3. Narrative Generation
   - Map mudras to meanings from knowledge base
   - Statistical aggregation (top 5 mudras)
   - Category analysis (single-hand vs. double-hand)

Phase 4: Deployment

1. API Development (FastAPI)
   - POST /analyze: Upload video
   - GET /result/{task_id}: Retrieve results
   - Background task processing

2. Cloud Deployment (Render)
   - Singleton model loading (memory optimization)
   - Concurrency control (threading lock)
   - Resource monitoring (psutil)

=================================================
SLIDE 6: SYSTEM DESIGN
=================================================

ARCHITECTURE OVERVIEW:

┌─────────────────────────────────────────────────────┐
│                   INPUT VIDEO                        │
│              (MP4, AVI, MOV, MKV, WEBM)             │
└────────────────┬────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────┐
│            FRAME EXTRACTION MODULE                   │
│        Process every 3 seconds (uniform)             │
└────────────────┬────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────┐
│         MEDIAPIPE FEATURE EXTRACTOR                  │
├──────────────────┬──────────────────────────────────┤
│  Hand Landmarks  │  Pose Landmarks  │  Emotion       │
│  (126 features)  │  (99 features)   │  (1 feature)   │
└──────────────────┴──────────────────┴────────────────┘
         │                    │
         ▼                    ▼
┌──────────────────┐  ┌──────────────────────────┐
│  MUDRA DETECTION │  │  DANCE STEP PREDICTION   │
│    (Static CNN)  │  │   (Temporal LSTM/GRU)    │
├──────────────────┤  ├──────────────────────────┤
│ 1. Hand Cropping │  │ 1. Feature Buffer (30)   │
│ 2. Resize 224×224│  │ 2. Sequence Modeling     │
│ 3. EfficientNetB0│  │ 3. Step Classification   │
│ 4. Top-K Preds   │  │ 4. Temporal Segments     │
└──────────────────┘  └──────────────────────────┘
         │                    │
         └──────────┬─────────┘
                    ▼
         ┌─────────────────────┐
         │ NARRATIVE GENERATOR  │
         ├─────────────────────┤
         │ • Mudra Meanings KB │
         │ • Statistical Aggr. │
         │ • Story Synthesis   │
         └─────────────────────┘
                    │
                    ▼
         ┌─────────────────────┐
         │  OUTPUT GENERATION  │
         ├─────────────────────┤
         │ • JSON Timeline     │
         │ • Text Narrative    │
         │ • Confidence Scores │
         └─────────────────────┘

MODULE DESCRIPTIONS:

1. Feature Extraction Module
   - MediaPipe Hands: Real-time 21-point hand tracking
   - MediaPipe Pose: Full-body pose estimation (33 points)
   - DeepFace: Facial emotion recognition
   - Hand Cropping: Bounding box extraction for CNN input

2. Mudra Classification Module
   - Model: EfficientNetB0 (Transfer Learning)
   - Input: 224×224 RGB hand crops
   - Output: 50 mudra classes with confidence scores
   - Preprocessing: Center-crop, edge padding, RGB conversion

3. Dance Step Prediction Module (Future)
   - Model: LSTM/GRU Sequence Model
   - Input: 30-frame window of combined features
   - Output: Dance step labels (Alarippu, Jathiswaram, etc.)

4. Narrative Generation Module
   - Knowledge Base: 50+ mudra meanings and categories
   - Functions: Statistical analysis, top-K selection
   - Output: Human-readable performance narrative

5. API Layer
   - Framework: FastAPI (async, high-performance)
   - Endpoints: /analyze, /result/{task_id}, /status
   - Features: Background tasks, memory optimization, concurrency control

DATA FLOW:

Video Upload → Task Queue → Frame Extraction → Feature Extraction
    → Mudra Detection → Temporal Aggregation → Narrative Generation
    → JSON + Text Output → User Download

TECHNOLOGY STACK:

Backend:
• Python 3.9/3.10
• TensorFlow 2.15+
• FastAPI + Uvicorn

Computer Vision:
• MediaPipe 0.10.9+
• OpenCV 4.8.0+
• DeepFace 0.0.79+

ML & Data:
• NumPy, Pandas
• Scikit-learn
• Matplotlib, Seaborn

Deployment:
• Render (Cloud Platform)
• Docker (Containerization)
• Git (Version Control)

=================================================
SLIDE 7: REFERENCES
=================================================

RESEARCH PAPERS:

1. Tan, M., & Le, Q. (2019). "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks." 
   International Conference on Machine Learning (ICML).

2. Zhang, F., Bazarevsky, V., Vakunov, A., et al. (2020). 
   "MediaPipe Hands: On-device Real-time Hand Tracking." 
   arXiv preprint arXiv:2006.10214.

3. Serengil, S. I., & Ozpinar, A. (2020). 
   "LightFace: A Hybrid Deep Face Recognition Framework." 
   IEEE International Conference on Innovations in Intelligent Systems and Applications (INISTA).

DATASETS:

4. Kaggle: "50 Mudras Dataset for Bharatanatyam" 
   https://www.kaggle.com/datasets/mudras-bharatanatyam

5. Custom Video Dataset: Bharatanatyam Performance Collection
   (Curated from YouTube and dance academies)

TOOLS & FRAMEWORKS:

6. MediaPipe Documentation: https://google.github.io/mediapipe/
7. TensorFlow Hub: https://www.tensorflow.org/hub
8. FastAPI Documentation: https://fastapi.tiangolo.com/

CULTURAL REFERENCES:

9. Bharatanatyam Mudras Encyclopedia
   - Traditional mudra classifications and meanings
   - Asamyuktha Hasta (28 single-hand mudras)
   - Samyuktha Hasta (22 double-hand mudras)

10. Natya Shastra - Ancient Indian Treatise on Performing Arts
    - Foundation for classical dance theory

=================================================
END OF PPT CONTENT
=================================================
